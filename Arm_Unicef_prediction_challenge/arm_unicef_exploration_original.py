# -*- coding: utf-8 -*-
"""Arm_Unicef_exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F2JF-qLDTszWexvf05okNStefAUu3cEr
"""

#!pip install albumentations==0.4.6

#from google.colab import drive
#drive.mount('/content/drive')

import re
#importing necessary librairies

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import time
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import albumentations as alb
from albumentations.pytorch.transforms import ToTensorV2
import torch
from torch.utils.data import DataLoader,Dataset
from torch.utils.data import SubsetRandomSampler
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection import FasterRCNN
from matplotlib import pyplot
import cv2
from tqdm import tqdm
import torch.nn as nn
import imgaug.augmenters as iaa


#!unzip "/content/drive/MyDrive/Images.zip"

train = pd.read_csv('Train.csv')
test = pd.read_csv('Test.csv')
submit= pd.read_csv('SampleSubmission.csv')

train=train.fillna(0)

"""Creating a class to pre-process the train csv file. 
We have about 2307 files that are useless!. They don't depict any visuals of the classes from the data. 
Moreover ,we will convert the bbox  "string format" to actual bounding boxes (a list)."""

class preprocess_train_csv:

  def decode_points(self,ddbox):
    points = [np.float32(point) for point in re.findall(r'\d+\.\d+', ddbox)]
    return points
  def preprocessor(self, data):

    ## data is actually the train csv file
    ## for the test we'll be using the sample
    ## submission data

    ## filtering out only the data with *not nan* labels
    train= data[data.category_id!=0.0].copy()
    ## just to confirm...
    print(f"length of train data is now {len(train)}")

    #now let's work with the bboxes.

    #The bbox column is a string, so we are going to decode the data points using regex

    train['processed_bbox']=train['bbox'].map(lambda x: self.decode_points(x))
    train['xmin']=train['processed_bbox'].map(lambda x: x[0])
    train['ymin']=train['processed_bbox'].map(lambda x: x[1])
    train['width']=train['processed_bbox'].map(lambda x: x[2])
    train['height']=train['processed_bbox'].map(lambda x: x[3])

    del train['processed_bbox']
    final_train=train[(train.width!=0.0)&(train.height!=0.0)].copy()
    return final_train

#train.category_id.value_counts()

len(submit)

preprocess= preprocess_train_csv()

train_new=preprocess.preprocessor(train)

train_new.head()

#append .tif to image ids for easier handling
train_new['image_id'] = train_new['image_id'].apply(lambda x: str(x) + '.tif')
train_new['bbox_area'] = train_new['width']*train_new['height']
train_new['category_id']=train_new['category_id'].map(lambda x: int(x))

train_new.category_id.value_counts()

#splitting into train and valid ids
train_split = 0.99

image_ids = train_new['image_id'].unique()
train_ids = image_ids[0:int(train_split*len(image_ids))]
val_ids = image_ids[int(train_split*len(image_ids)):]

print('Length of training ids', len(train_ids))
print('Length of validation ids', len(val_ids))

train_df = train_new[train_new['image_id'].isin(train_ids)]
valid_df = train_new[train_new['image_id'].isin(val_ids)]

classes_la=['Tin','Thatch','others']

"""### Training procedure (using Faster-RCNN)"""

class Unicef_Image_Dataset(Dataset):
    def __init__(self, df, image_dir,transform = None,mixup_alpha=0.4):
        super().__init__()
        self.df = df
        self.img_ids = df['image_id'].unique()
        self.image_dir = image_dir
        self.transform = transform
        self.mixup_alpha = mixup_alpha

        # Group image indices by their dimensions
        self.dimension_groups = self._group_images_by_dimensions()
    def __len__(self):
        return len(self.img_ids)
        
    def _group_images_by_dimensions(self):
        dimension_groups = {}
        for image_id in self.df['image_id'].unique():
            image_path = os.path.join(self.image_dir, image_id)
            image = cv2.imread(image_path, cv2.IMREAD_COLOR)
            height, width, _ = image.shape
            dimension = (height, width)
            if dimension not in dimension_groups:
                dimension_groups[dimension] = []
            dimension_groups[dimension].append(image_id)
        return dimension_groups
        
    def __getitem__(self, idx: int):
        image_id = self.img_ids[idx]
        pts = self.df[self.df['image_id'] == image_id]

        image = cv2.imread(os.path.join(self.image_dir, image_id), cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
        image = image/255.0

        boxes = pts[['xmin', 'ymin', 'width', 'height']].values

        #convert boxes to x1,y1,x2,y2 format because that is what resnet50 faster cnn in pytorch expects
        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]
        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]

        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) #width times height
        area = torch.as_tensor(area, dtype = torch.float32)

        # there are 3 classes
        labels=pts["category_id"].values
        labels=torch.tensor(labels)
        #labels = torch.ones((pts.shape[0],), dtype=torch.int64)

        # background instance
        iscrowd = torch.zeros(len(classes_la), dtype=torch.int32) #(pts.shape[0],)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = torch.tensor(idx)
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transform:
            sample = {
                'image': image,
                'bboxes': target['boxes'],
                'labels': target['labels']
            }
            sample = self.transform(**sample)
            image = sample['image']

            if len(sample['bboxes']) > 0:
                target['boxes'] = torch.as_tensor(sample['bboxes'], dtype=torch.float32)
            else:
                target['boxes'] = torch.linspace(0,3, steps = 4, dtype = torch.float32)
                target['boxes'] = target['boxes'].reshape(-1,4)
                
                # Mixup implementation within dimension group
        if self.mixup_alpha > 0 and np.random.rand() < 0.5:
            height, width, _ = image.shape
            dimension = (height, width)
            if dimension in self.dimension_groups:
                mix_idx = np.random.choice(self.dimension_groups[dimension])
                if mix_idx != image_id:
                    mix_pts = self.df[self.df['image_id'] == mix_idx]
                    mix_image = cv2.imread(os.path.join(self.image_dir, mix_idx), cv2.IMREAD_COLOR)
                    mix_image = cv2.cvtColor(mix_image, cv2.COLOR_BGR2RGB).astype(np.float32)
                    mix_image = mix_image / 255.0

                    mix_boxes = mix_pts[['xmin', 'ymin', 'width', 'height']].values
                    mix_boxes[:, 2] = mix_boxes[:, 0] + mix_boxes[:, 2]
                    mix_boxes[:, 3] = mix_boxes[:, 1] + mix_boxes[:, 3]

                    mix_labels = mix_pts["category_id"].values

                    lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)
                    image = lam * image + (1 - lam) * mix_image

                    target['boxes'] = np.concatenate((target['boxes'], mix_boxes), axis=0)
                    target['labels'] = torch.cat((target['labels'], torch.tensor(mix_labels)), dim=0)
                    target['area'] = torch.cat((target['area'], torch.as_tensor((mix_boxes[:, 3] - mix_boxes[:, 1]) * (mix_boxes[:, 2] - mix_boxes[:, 0]), dtype=torch.float32)), dim=0)
                    target['iscrowd'] = torch.cat((target['iscrowd'], torch.zeros((mix_boxes.shape[0],), dtype=torch.int64)), dim=0)

        return image, target, image_id

#defining the transformations
# heavy augmentation
def get_training_transforms():
    return alb.Compose([
    alb.VerticalFlip(p = 0.5),
    alb.HorizontalFlip(p = 0.5),
    alb.RandomBrightnessContrast(p = 1),
    ToTensorV2(p = 1.0)
], p=1.0, bbox_params=alb.BboxParams(format='pascal_voc', label_fields=['labels']))

def get_validation_transforms():
    return alb.Compose([ToTensorV2(p = 1.0)], p = 1.0, bbox_params = alb.BboxParams(format='pascal_voc', label_fields=['labels']))

# load a pre-trained model for classification and return
# only the features

densenet_net = torchvision.models.resnet18(pretrained=True)
# FasterRCNN needs to know the number of
# output channels in a backbone. For mobilenet_v2, it's 1280
# so we need to add it here
modules = list(densenet_net.children())[:-2]
backbone = nn.Sequential(*modules)
backbone.out_channels = 512# for resnet18 is 512, for mobilenet_v2 is 1280


# let's make the RPN generate 5 x 3 anchors per spatial
# location, with 5 different sizes and 3 different aspect
# ratios. We have a Tuple[Tuple[int]] because each feature
# map could potentially have different sizes and
# aspect ratios
anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),
                                   aspect_ratios=((0.5, 1.0,2.0,2.5),))

# let's define what are the feature maps that we will
# use to perform the region of interest cropping, as well as
# the size of the crop after rescaling.
# if your backbone returns a Tensor, featmap_names is expected to
# be [0]. More generally, the backbone should return an
# OrderedDict[Tensor], and in featmap_names you can choose which
# feature maps to use.
roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=["0"],
                                                output_size=7,
                                                sampling_ratio=2)

# put the pieces together inside a FasterRCNN model
model = FasterRCNN(backbone,
                   num_classes=4,
                   rpn_anchor_generator=anchor_generator,
                   box_roi_pool=roi_pooler)

def collate_fn(batch):
    return tuple(zip(*batch))

#setting up dataloaders
train_img_path="/users/allassan/Images/"
training_dataset = Unicef_Image_Dataset(train_df, train_img_path, get_training_transforms(),mixup_alpha=0.7)
validation_dataset = Unicef_Image_Dataset(valid_df, train_img_path, get_validation_transforms(),mixup_alpha=0.7)

train_dataloader = DataLoader(
        training_dataset, batch_size=2, shuffle= True, num_workers=4,
        collate_fn= collate_fn)

valid_dataloader = DataLoader(
        validation_dataset, batch_size=2, shuffle=False, num_workers=4,
        collate_fn=collate_fn)

# train on the GPU or on the CPU, if a GPU is not available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
torch.cuda.empty_cache()
device


model.to(device)
# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]

optimizer = torch.optim.SGD(params, lr= 0.01, momentum=0.9, dampening=0, weight_decay=0, nesterov=False)

# and a learning rate scheduler
lr_scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True, threshold=0.0001, threshold_mode='abs', cooldown=0, min_lr=1e-8, eps=1e-08)
# let's train it for 10 epochs

num_epochs = 10

total_train_loss = []
total_test_loss = []
itr = 1

for epoch in range(num_epochs):
    model.train()

    print('Epoch: ', epoch + 1)
    train_loss = []

    for images, targets, image_ids in tqdm(train_dataloader):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        loss_value = losses.item()
        train_loss.append(loss_value)

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()



    epoch_loss = np.mean(train_loss)
    print('Epoch Loss is: ' , epoch_loss)
    total_train_loss.append(epoch_loss)

    with torch.no_grad():
        test_losses = []
        for images, targets, image_ids in tqdm(valid_dataloader):
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            loss_dict = model(images, targets)

            losses = sum(loss for loss in loss_dict.values())
            test_loss = losses.item()
            test_losses.append(test_loss)

    test_losses_epoch = np.mean(test_losses)
    print('Test Loss: ' ,test_losses_epoch)
    total_test_loss.append(test_losses_epoch)

    if lr_scheduler is not None:
        lr_scheduler.step(test_losses_epoch)

torch.save(model.state_dict(), f'fasterrcnn_{epoch}.pth')

"""## Inference Time"""

#!unzip "/content/drive/MyDrive/Test.zip"

class Unicef_Dataset_Test(Dataset):
    def __init__(self, id_list, image_dir,transform = None):
        super().__init__()
        self.img_ids = id_list
        self.image_dir = image_dir
        self.transform = transform

    def __len__(self):
        return len(self.img_ids)

    def __getitem__(self, idx: int):
        image_id = self.img_ids[idx]
        #print(image_id)
        image = cv2.imread(os.path.join(self.image_dir, image_id), cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
        image = image/255.0

        if self.transform:
            sample = {
                'image': image
            }
            sample = self.transform(**sample)
            image = sample['image']

        return image,image_id

def get_test_transforms():
    return alb.Compose([ToTensorV2(p = 1.0)], p = 1.0)

paths=['/users/allassan/fasterrcnn_9.pth']
test_img_path= "/users/allassan/Test/"

test_images=os.listdir("/users/allassan/Test/")

submit.head()

for path_of_model in paths:
  model = FasterRCNN(backbone,
                   num_classes=4,
                   rpn_anchor_generator=anchor_generator,
                   box_roi_pool=roi_pooler)
  model.load_state_dict(torch.load(path_of_model,map_location=torch.device('cpu')))
  model.eval()
  if torch.cuda.is_available():
    model.cuda()
  test_dataset=Unicef_Dataset_Test(test_images,test_img_path,get_test_transforms())
  results=[]
  for j in range(0,len(os.listdir("/users/allassan/Test/"))):
    img,_ = test_dataset[j]
    img = img.unsqueeze_(0)
    model.eval()

    with torch.no_grad():
      prediction = model([img.to(device)][0])
      thresh = 0.80   #with 0.4, we have 931 unique id's
      box_preds = prediction[0]['boxes'].cpu().detach().numpy()
      score_preds = prediction[0]['scores'].cpu().detach().numpy()
      labels=prediction[0]['labels'].cpu().detach().numpy()

      box_preds = box_preds[score_preds >= thresh].astype(np.float32)
      score_preds=score_preds[score_preds >= thresh]
      labels=labels[:len(score_preds)]
      #print(labels)
      #aa = zip(box_preds.tolist(),labels, score_preds.tolist())
      for idx in range(1,4):
        row_dict = {}
        row_dict["image_id"] = _[:-4]+'_'+str(idx)
        row_dict["Target_2"] = labels.tolist().count(idx)
        results.append(row_dict)
    sub_df = pd.DataFrame(results)

sub_df.head()

submit.head()

# Merge the DataFrames on the 'image_id' column
merged_df = pd.merge(submit, sub_df, on='image_id', how='left')

# Update the 'Value1' column in the first DataFrame with 'Value2' from the second DataFrame
submit['Target'] = merged_df['Target_2'].combine_first(submit['Target'])

submit['Target']=submit['Target'].map(lambda x: int(x))

submit.to_csv('final_submission.csv',index=False)

